# -*- coding: utf-8 -*-
"""training.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/18AaWJzLAWAfI5FR25eW65CWtsMatdiUV
"""

#%%
# from google.colab import drive
import fiftyone as fo
import fiftyone.types as fot
import numpy as np
from PIL import Image
import jax
import jax.numpy as jnp
from flax import linen as nn
from flax.training import train_state
import optax
import sys
from typing import Any

print(jax.devices())
#%%
sys.setrecursionlimit(3000)

#%%
drive.mount('/content/drive')

#%%
dataset_name = "Voxel51/PKLot"

# Check if it exists and delete it
if fo.dataset_exists(dataset_name):
    fo.delete_dataset(dataset_name)
    print(f"Deleted existing dataset: {dataset_name}")

#%%
dataset_dir = "PKLot"

dataset = fo.Dataset.from_dir(
    dataset_dir=dataset_dir,
    dataset_type=fo.types.FiftyOneDataset,
)

#%%
session = fo.launch_app(dataset, auto=False)

session.show()

#%%
# Preprocessing
def preprocess_sample(sample):
    # Resize image to 416x416
    img = Image.open(sample.filepath).resize((416, 416))
    img_array = np.array(img) / 255.0  # Normalize to [0,1]

    # Extract bounding boxes and labels from occupied parking spaces
    bboxes = []
    gt_labels = []  # 1 for car
    if sample.parking_spaces is not None:
        for polyline in sample.parking_spaces.polylines:
            if polyline.occupancy_status == "occupied":
                points = polyline.points[0]  # First polygon
                if not points:  # Skip empty points list
                    print(f"Warning: Empty points in occupied polyline for sample {sample.filepath}")
                    continue
                try:
                    x_min = min(x for x, _ in points)
                    y_min = min(y for _, y in points)
                    x_max = max(x for x, _ in points)
                    y_max = max(y for _, y in points)
                    bbox = [x_min, y_min, x_max - x_min, y_max - y_min]  # Normalized [x, y, w, h]
                    bboxes.append(bbox)
                    gt_labels.append(1.0)
                except ValueError as e:
                    print(f"Error processing points in sample {sample.filepath}: {e}")
                    continue

    return img_array, np.array(bboxes), np.array(gt_labels)

#%%
# Create splits
dataset.shuffle(seed=42)
n_samples = len(dataset)
n_train = int(0.8 * n_samples)
n_val = int(0.1 * n_samples)

dataset.take(n_train).tag_samples("train")
dataset.skip(n_train).take(n_val).tag_samples("validation")
dataset.skip(n_train + n_val).tag_samples("test")

# Fix: Count samples by tags
print("Split sizes:",
      dataset.match_tags("train").count(),
      dataset.match_tags("validation").count(),
      dataset.match_tags("test").count())

#%%
class ModifiedYOLOv1(nn.Module):
    grid_size: int = 13  # 13x13 grid
    num_boxes: int = 2  # 2 boxes per cell
    num_classes: int = 1  # "car"

    @nn.compact
    def __call__(self, x, train: bool = True):
        # Simplified 10-layer backbone (inspired by YOLOv1 Darknet)
        x = nn.Conv(64, kernel_size=(7, 7), strides=(2, 2), padding='SAME')(x)
        x = nn.BatchNorm(use_running_average=not train)(x)
        x = jax.nn.leaky_relu(x)
        x = nn.max_pool(x, window_shape=(2, 2), strides=(2, 2))  # 416/4 = 104

        x = nn.Conv(192, kernel_size=(3, 3), padding='SAME')(x)
        x = nn.BatchNorm(use_running_average=not train)(x)
        x = jax.nn.leaky_relu(x)
        x = nn.max_pool(x, window_shape=(2, 2), strides=(2, 2))  # 104/2 = 52

        x = nn.Conv(128, kernel_size=(1, 1), padding='SAME')(x)
        x = nn.BatchNorm(use_running_average=not train)(x)
        x = jax.nn.leaky_relu(x)

        x = nn.Conv(256, kernel_size=(3, 3), padding='SAME')(x)
        x = nn.BatchNorm(use_running_average=not train)(x)
        x = jax.nn.leaky_relu(x)

        x = nn.Conv(256, kernel_size=(1, 1), padding='SAME')(x)
        x = nn.BatchNorm(use_running_average=not train)(x)
        x = jax.nn.leaky_relu(x)

        x = nn.Conv(512, kernel_size=(3, 3), padding='SAME')(x)
        x = nn.BatchNorm(use_running_average=not train)(x)
        x = jax.nn.leaky_relu(x)
        x = nn.max_pool(x, window_shape=(2, 2), strides=(2, 2))  # 52/2 = 26

        x = nn.Conv(256, kernel_size=(1, 1), padding='SAME')(x)
        x = nn.BatchNorm(use_running_average=not train)(x)
        x = jax.nn.leaky_relu(x)

        x = nn.Conv(512, kernel_size=(3, 3), padding='SAME')(x)
        x = nn.BatchNorm(use_running_average=not train)(x)
        x = jax.nn.leaky_relu(x)

        x = nn.Conv(512, kernel_size=(3, 3), padding='SAME')(x)
        x = nn.BatchNorm(use_running_average=not train)(x)
        x = jax.nn.leaky_relu(x)

        x = nn.Conv(1024, kernel_size=(3, 3), strides=(2, 2), padding='SAME')(x)  # 26/2 = 13
        x = nn.BatchNorm(use_running_average=not train)(x)
        x = jax.nn.leaky_relu(x)

        # Detection head: 1x1 conv for grid output (13x13x(2*(4+1)+1) = 13x13x11)
        out_channels = self.num_boxes * (4 + 1) + self.num_classes  # 11
        x = nn.Conv(out_channels, kernel_size=(1, 1), padding='SAME')(x)
        return x
    
# TrainState for managing params and batch_stats
class TrainState(train_state.TrainState):
    batch_stats: Any

#%%
# Initialize model
model = ModifiedYOLOv1()
rng = jax.random.PRNGKey(0)
variables = model.init(rng, jnp.ones((1, 416, 416, 3)), train=True)
optimizer = optax.adam(1e-3)
state = TrainState.create(
    apply_fn=model.apply,
    params=variables['params'],
    batch_stats=variables.get('batch_stats', {}),
    tx=optimizer
)
#%%
# Create batch
def create_batch(view, batch_size=8, grid_size=13, num_boxes=2, num_classes=1):
    samples = view.take(batch_size)
    images = []
    gt_grid = []
    for sample in samples:
        img, bboxes, labels = preprocess_sample(sample)
        images.append(img)
        grid = np.zeros((grid_size, grid_size, num_boxes * (4 + 1) + num_classes))
        cell_size = 1.0 / grid_size
        for bbox, label in zip(bboxes, labels):
            x, y, w, h = bbox
            cx, cy = x + w / 2, y + h / 2  # Center
            cell_x, cell_y = int(cx / cell_size), int(cy / cell_size)
            if cell_x >= grid_size or cell_y >= grid_size:
                continue
            # Assign to first box slot (simplified, no IoU-based assignment)
            offset = 0 * (4 + 1)
            grid[cell_y, cell_x, offset:offset+4] = [cx % cell_size, cy % cell_size, np.sqrt(w), np.sqrt(h)]
            grid[cell_y, cell_x, offset+4] = 1.0  # Confidence
            grid[cell_y, cell_x, -num_classes:] = label  # Class
        gt_grid.append(grid)
    return {
        'image': jnp.array(images, dtype=jnp.float32),
        'gt_grid': jnp.array(gt_grid, dtype=jnp.float32)
    }

# YOLOv1 Loss
def yolo_loss(pred_grid, gt_grid, coord_weight=5.0, noobj_weight=0.5):
    pred_grid = pred_grid.reshape(gt_grid.shape)  # [batch, grid_size, grid_size, 11]
    num_boxes = 2
    num_classes = 1
    box_size = 4 + 1  # x,y,w,h,conf
    pred_boxes = pred_grid[..., :num_boxes*box_size].reshape(-1, num_boxes, box_size)
    pred_class = pred_grid[..., -num_classes:]
    gt_boxes = gt_grid[..., :num_boxes*box_size].reshape(-1, num_boxes, box_size)
    gt_class = gt_grid[..., -num_classes:]
    obj_mask = gt_boxes[..., 4] > 0
    noobj_mask = ~obj_mask
    box_diff = pred_boxes[..., :4] - gt_boxes[..., :4]
    box_loss = coord_weight * jnp.mean(jnp.square(box_diff) * obj_mask[..., None])
    conf_diff = pred_boxes[..., 4] - gt_boxes[..., 4]
    conf_obj_loss = jnp.mean(jnp.square(conf_diff) * obj_mask)
    conf_noobj_loss = noobj_weight * jnp.mean(jnp.square(conf_diff) * noobj_mask)
    conf_loss = conf_obj_loss + conf_noobj_loss
    class_loss = jnp.mean(optax.sigmoid_binary_cross_entropy(pred_class, gt_class))
    return box_loss + conf_loss + class_loss

# Training step
def train_step(state, batch):
    def loss_fn(params):
        pred_grid, new_batch_stats = state.apply_fn(
            {'params': params, 'batch_stats': state.batch_stats},
            batch['image'],
            train=True,
            mutable=['batch_stats']
        )
        loss = yolo_loss(pred_grid, batch['gt_grid'])
        # Debug: Log new_batch_stats structure
        print(f"New batch_stats keys: {list(new_batch_stats.get('batch_stats', {}).keys())}")
        return loss, new_batch_stats
    (loss, new_batch_stats), grads = jax.value_and_grad(loss_fn, has_aux=True)(state.params)
    # Manually merge new_batch_stats into existing batch_stats
    current_batch_stats = state.batch_stats
    if 'batch_stats' in new_batch_stats:
        for key, value in new_batch_stats['batch_stats'].items():
            if key in current_batch_stats:
                current_batch_stats[key] = value  # Update in place
            else:
                current_batch_stats[key] = value  # Add new key
    state = state.apply_gradients(grads=grads, batch_stats=current_batch_stats)
    return state, loss

# mAP computation
def compute_map(model, state, view, grid_size=13, iou_thres=0.5):
    from sklearn.metrics import average_precision_score
    def giou_loss(pred_boxes, gt_boxes):
        lt = jnp.maximum(pred_boxes[:, None, :2], gt_boxes[None, :, :2])
        rb = jnp.minimum(pred_boxes[:, None, 2:], gt_boxes[None, :, 2:])
        wh = jnp.maximum(rb - lt, 0.0)
        inter = wh[:, :, 0] * wh[:, :, 1]
        area_pred = (pred_boxes[:, 2] - pred_boxes[:, 0]) * (pred_boxes[:, 3] - pred_boxes[:, 1])
        area_gt = (gt_boxes[:, 2] - gt_boxes[:, 0]) * (gt_boxes[:, 3] - gt_boxes[:, 1])
        union = area_pred[:, None] + area_gt[None, :] - inter
        enclosed_lt = jnp.minimum(pred_boxes[:, None, :2], gt_boxes[None, :, :2])
        enclosed_rb = jnp.maximum(pred_boxes[:, None, 2:], gt_boxes[None, :, 2:])
        enclosed_wh = jnp.maximum(enclosed_rb - enclosed_lt, 0.0)
        enclosed_area = enclosed_wh[:, :, 0] * enclosed_wh[:, :, 1]
        iou = jnp.where(union > 0, inter / union, jnp.zeros_like(inter))
        giou = iou - (enclosed_area - union) / enclosed_area
        return 1 - giou
    
    ap_scores = []
    for sample in view:
        img, gt_boxes, gt_labels = preprocess_sample(sample)
        pred_grid = state.apply_fn(
            {'params': state.params, 'batch_stats': state.batch_stats},
            jnp.expand_dims(img, 0),
            train=False
        )
        pred_grid = pred_grid.reshape(grid_size, grid_size, -1)
        pred_boxes = []
        pred_scores = []
        cell_size = 1.0 / grid_size
        for i in range(grid_size):
            for j in range(grid_size):
                for b in range(2):
                    offset = b * 5
                    conf = jax.nn.sigmoid(pred_grid[i, j, offset+4])
                    if conf < 0.5:
                        continue
                    x = (i + pred_grid[i, j, offset]) * cell_size
                    y = (j + pred_grid[i, j, offset+1]) * cell_size
                    w = (pred_grid[i, j, offset+2])**2
                    h = (pred_grid[i, j, offset+3])**2
                    pred_boxes.append([x - w/2, y - h/2, x + w/2, y + h/2])  # [x1, y1, x2, y2]
                    pred_scores.append(conf)
        if len(gt_boxes) == 0 or len(pred_boxes) == 0:
            continue
        gt_boxes_xyxy = np.array([[b[0] - b[2]/2, b[1] - b[3]/2, b[0] + b[2]/2, b[1] + b[3]/2] for b in gt_boxes])
        pred_boxes = jnp.array(pred_boxes)
        iou = giou_loss(pred_boxes, jnp.array(gt_boxes_xyxy)) - 1  # IoU
        max_iou = iou.max(axis=1)
        y_true = (max_iou >= iou_thres).astype(int)
        y_score = pred_scores[:len(y_true)]
        ap = average_precision_score(y_true, y_score)
        ap_scores.append(ap)
    return np.mean(ap_scores) if ap_scores else 0.0

#%%
# Train
for epoch in range(20):
    train_view = dataset.match_tags("train")
    for i in range(0, len(train_view), 8):
        batch = create_batch(train_view.skip(i).take(8))
        state, loss = train_step(state, batch)
        print(f"Epoch {epoch}, Batch {i//8}, Loss: {loss}")
    
    # Compute validation mAP every 5 epochs
    if (epoch + 1) % 5 == 0:
        val_view = dataset.match_tags("validation")
        mAP = compute_map(model, state, val_view)
        print(f"Epoch {epoch + 1}, Validation mAP@0.5: {mAP}")
        if mAP > 0.7:
            print(f"Stopping early at epoch {epoch + 1} with mAP {mAP}")
            break
# %%
import pickle 
import os
save_dir = os.path.join(dataset_dir, "saved_model")
os.makedirs(save_dir, exist_ok=True)
save_path = os.path.join(save_dir, "model_params.pkl")
model_state = {
    'params': state.params,
    'batch_stats': state.batch_stats
}
with open(save_path, 'wb') as f:
    pickle.dump(model_state, f)
print(f"Model parameters saved to {save_path}")
